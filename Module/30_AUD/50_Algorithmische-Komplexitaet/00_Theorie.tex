\documentclass{bschlangaul-haupt}
\liLadePakete{syntax,mathe,formale-sprachen}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorie-Teil
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Algorithmische Komplexität}

\inhaltsverzeichnis

\begin{liQuellen}
\item \cite[Seite 196-206 (PDF 214-224)]{saake}
\item \cite[Seite 19-35 (PDF 11-24)]{aud:fs:2}
\end{liQuellen}

\noindent
Die Algorithmische Komplexität macht Aussagen über den
\memph{Ressourcenverbrauch} (Laufzeit und Resourcenverbrauch) eines
Algorithmus. Aussagen zur Algorithmischen Komplexität sind unabhängig
von:

\begin{compactitem}
\item Programmiersprache
\item installiertem Betriebssystem
\item Prozessorleistung
\item Speicherausstattung
\item zugrunde liegender Computerarchitektur
\end{compactitem}

\noindent
Die Algorithmische Komplexität beschreibt die \memph{Effizienz} eines
eingesetzten Algorithmus und entscheidet über die \memph{praktische
Anwendbarkeit} eines Verfahrens.
\footcite[Seite 20 (PDF 12)]{aud:fs:2}

%-----------------------------------------------------------------------
%
%-----------------------------------------------------------------------

\subsection{Zeitkomplexität}

Die Speicherkapazität wächst schneller als die Taktfrequenz. Die Zeit
ist die meist beschränkende Größe.
%
Die Anzahl der für seine Ausführung benötigten elementaren Operationen
eines Algorithmus in Abhängigkeit von der Eingabelänge $n$ heißt,
Laufzeit $T(n)$.
%
Ziel ist die Laufzeitanalyse für große Eingaben. Mit Fachbegriff nennt
man diesen Sachverhalt \emph{„asymptotische Komplexität“}.
\footcite[Seite 21 (PDF 12)]{aud:fs:2}

Da der Aufwand schon vom Ansatz her ohnehin nur grob abgeschätzt werden
kann, brauchen einige Informationen, wie etwa Konstanten nicht
berücksichtig werden.

 $T_n = \mathcal{O}(\log n)$, bedeutet zum Beispiel, dass $T_n$
\emph{„von der Ordnung log n“} ist, wobei multiplikative und additive
Konstanten sowie die Basis des Logarithmus unspezifiziert bleiben.
\footcite[Seite 200 (PDF 2018)]{saake}

Da die $\mathcal{O}(g)$-Notation eine obere Grenze für eine Klasse von
Funktionen definiert, kann die Funktion $g$ jeweils vereinfacht werden,
indem konstante Faktoren weggelassen werden sowie nur der höchste
Exponent berücksichtigt wird.
\footcite[Seite 201 (PDF 2019)]{saake}

\section{$\mathcal{O}$-Notation}

Offensichtlich gilt für große $n$:
$1 < \log_2(n) < n < n^2 < n^3 < ... < 2^n$

Bestimmung einer asymptotischen oberen Schranke $g$.

$T(n) \in \mathcal{O}(g(n))$ bedeutet:

\begin{itemize}
\item $T(n)$ wächst höchstens so schnell wie die Funktion $g(n)$

\item Es gibt eine Konstante $c$, sodass für große $n$ immer gilt: $T(n)
\leq c \cdot g(n)$

\item $c \cdot g(n)$ ist damit eine obere Schranke für $T(n)$
\end{itemize}

\section{Definition (O-Notation)}

$\mathcal{O}(g(n)) = \{T(n)|$
Es gibt ein
$c > 0$
und ein
$n_0$
aus,
$\mathbb{N}$
sodass gilt:
$0 \leq T(n) \leq c \cdot g(n)$
für alle
$n \geq n_0\}$

$\mathcal{O}(g(n)) = \{T(n)|
\exists
c > 0,
n_0 \in \mathbb{N}:
\forall n \geq n_0 0 \leq T(n) \leq c \cdot g(n)
\}$

\begin{minted}{java}
max = messwert[0]; // 1
for (int i = 0; i < anzMesswerte; i++) { // n-mal
  System.out.println(i + ":" + messwert[i]); // konstanter Bedarf
  if (messwert[i] > max) {
    max = messwert[i];
  }
}
\end{minted}

\noindent
$T(n) = 1 + \text{const} * n$
$\rightarrow$ Laufzeit ist in $\mathcal{O}(n)$

\begin{minted}{java}
for (int k = 0; k < anzVersuchsreihen; k++) { // n-mal
  System.out.println(k + "-te Versuchsreihe: "); // konstant
  for ( int i = 0; i < anzMesswerte; i++) {
    System.out.println(k + "." + i + ":" + messwert[k][i]); // n-mal
  }
}
\end{minted}

\noindent
$T(n)= n + n^2$
$\rightarrow$ wächst quadratisch mit $n$
$\rightarrow$ $T(n)$ ist in $\mathcal{O}(n^2)$
\footcite[Seite 25 (PDF 17)]{aud:fs:2}

%%
%
%%

\subsection{Definition (Best Case, Worst Case, Average Case)}

Der günstigste Fall (\memph{best case}) für einen Algorithmus liegt vor,
wenn sein Ergebnis mit minimalem Aufwand erzielt werden kann.

Es handelt sich um den ungünstigsten Fall (\memph{worst case}), wenn der
Aufwand zur Erzielung des Ergebnisses maximal ist.

Im Durchschnittsfall (\memph{average case}) ist ein mittlerer erwarteter
Aufwand erforderlich.\footcite[Seite 26 (PDF 18)]{aud:fs:2}

\subsubsection{Beispiel}

Ist eine gegebene Zahl n eine Primzahl?

\begin{description}
\item[best case] $n = 14672940583676948672397956$
(Die Zahl ist gerade und daher durch zwei teilbar)

\item[worst case] $n$ ist eine Primzahl

\item[average case] (Um den \emph{average case} zu bestimmen, ist eine
aufwändige statistische Berechnung nötig.)
\end{description}

%%
%
%%

\subsection{Beispiel einer Komplexitätsberechnung}

\begin{minted}{java}
int summe = 0;
for (int j = 1 ; j < n ; j ++) {
  for (int i = 1 ; i < j ; i ++) {
    summe = summe + 1;
  }
}
for (int k = 0 ; k <= n ; k ++) {
  a[k] = k;
}
\end{minted}

\noindent
Offenbar fallen die Fälle \emph{Best Case}, \emph{Worst Case} und
\emph{Average Case} zusammen, da die Anzahl der Durchläufe der
Wiederholungsanweisungen fest ist.
\footcite[Seite 27 (PDF 19)]{aud:fs:2}

%%
%
%%

\subsubsection{ersten Zeile}

\begin{minted}{java}
int summe = 0;
\end{minted}

\noindent
Die Zuweisung in der ersten Zeile benötigt die konstante Zeit $c_1$,
also gilt: $T_{\text{Erste Zuweisung}}(n) = \mathcal{O}(1)$
\footcite[Seite 28 (PDF 20)]{aud:fs:2}

%%
%
%%

\subsubsection{erste for-Anweisung}

\begin{minted}[firstnumber=2]{java}
for (int j = 1 ; j < n ; j ++) {
  for (int i = 1 ; i < j ; i ++) {
    summe = summe + 1;
  }
}
\end{minted}

\noindent
Die innere Zuweisung benötigt einen konstanten Zeitbedarf $c_2$. Die
innere Wiederholungsanweisung eine $j$-fache Ausführung, \dh den
Zeitbedarf $j \cdot c_2$. Die äußere Wiederholungsanweisung eine
$n$-fache Ausführung. Bei jedem Durchlauf wird $j$ um 1 inkrementiert.
Die Laufzeit für die verschachtelte for-Anweisung beträgt:

\begin{displaymath}
c_2 \cdot (1 + 2 + \ldots + n) =
c_2 \cdot \frac{n \cdot (n + 1) }{2} =
c_2 \cdot \frac{(n^2 + n)}{2} =
c_2 / 2 \cdot (n^2 + n)
\end{displaymath}
\footcite{wiki:gausssche-summenformel}

\begin{displaymath}
\mathcal{O}(c_2 / 2 \cdot (n_2 + n)) =
\mathcal{O}(n^2 + n) =
\mathcal{O}(n^2)
\end{displaymath}
\footcite[Seite 29 (PDF 21)]{aud:fs:2}

%%
%
%%

\subsubsection{zweite for-Anweisung}
\noindent
Für die zweite for-Anweisung gilt:

\begin{minted}[firstnumber=7]{java}
for (int k = 0 ; k <= n ; k ++) {
  a[k] = k;
}
\end{minted}

\begin{displaymath}
T_\text{Zweite for-Anweisung}(n) = (n + 1) * c_3 = \mathcal{O}(n)
\end{displaymath}

\noindent
Laufzeitverhalten für das gesamte Codefragment:

\begin{align*}
T_\text{gesamt} &\\
= & T_\text{Erste Zuweisung}(n) +
    T_\text{Erste for-Anweisung}(n) +
    T_\text{Zweite for-Anweisung}(n) \\
= &
    \mathcal{O}(1) + \mathcal{O}(n^2) + \mathcal{O}(n)\\
= & \mathcal{O}(n^2)
\end{align*}

\noindent
Insgesamt ist die Laufzeit für das gesamte Codefragment also
quadratisch.
\footcite[Seite 30]{aud:fs:2}

%-----------------------------------------------------------------------
%
%-----------------------------------------------------------------------

\section{Typische Komplexitätsklassen}

Quellen
\footcite[Seite 24 (PDF 16)]{aud:fs:2}
\footcite[Seite 202 (PDF 220)]{saake}
\footcite[Absatz „Anforderungen an Schrankenfunktionen“]{wiki:komplexitaetstheorie}

\begin{tabular}{|l|l|}
konstant &$\mathcal{O}(1)$ \\
logarithmisch &$\mathcal{O}(\log n)$ \\
linear &$\mathcal{O}(n)$ \\
linearithmisch  &$\mathcal{O}(n \log n)$ \\
quadratisch &$\mathcal{O}(n^2)$ \\
polynomial &$\mathcal{O}(n^k)$ für $k \geq 1$ \\
exponentiell &$\mathcal{O}(d^n)$ für $d > 1$ \\
\end{tabular}

\section{Landau-Symbole}

Landau-Symbole werden in der Informatik verwendet, um das asymptotische
Verhalten von Funktionen und Folgen zu beschreiben. In der Informatik
werden sie bei der Analyse von Algorithmen verwendet und geben ein Maß
für die Anzahl der Elementarschritte oder der Speichereinheiten in
Abhängigkeit von der Größe der Eingangsvariablen an. Die
Komplexitätstheorie verwendet sie, um verschiedene Probleme danach zu
vergleichen, wie „schwierig“ oder aufwendig sie zu lösen sind. Man sagt,
„schwere Probleme“ wachsen exponentiell mit der Instanz oder schneller
und für „leichte Probleme“ existiert ein Algorithmus, dessen
Laufzeitzuwächse sich durch das Wachstum eines Polynoms beschränken
lassen. Man nennt sie (nicht) polynomiell lösbar.
\footcite{wiki:landau-symbole}

\begin{center}
\begin{tabular}{ll}
\textbf{Notation} &
\textbf{Anschauliche Bedeutung}
\\\hline

$f \in \begin{smallmatrix}\!\mathcal{O}\!\end{smallmatrix}(g)$ &
$f$ wächst langsamer als $g$
\\\hline

$f \in \mathcal{O}(g)$ &
$f$ wächst nicht wesentlich schneller als $g$
\\\hline

$f \in \Theta(g)$ &
$f$ wächst genauso schnell wie $g$
\\\hline

$f \in \Omega(g)$ &
$f$ wächst nicht wesentlich langsamer als $g$
\\\hline

$f \in \omega(g)$ &
$f$ wächst schneller als $g$
\\\hline
\end{tabular}
\end{center}

\subsection{Asymptotische \emph{untere} Schranke: $\Omega$-Notation}

\begin{compactitem}
\item $f(n) \in \Omega(g(n))$: $f(n)$ wächst mindestens so schnell wie $g(n)$
\item \liAusdruck[\Omega(g(n))]
{f(n)}
{\exists c > 0 \exists n_0 > 0 \forall n \geq n_0: 0 \leq c \cdot g(n) \leq f(n)}
\end{compactitem}

\subsection{Asymptotisch \emph{scharfe} Schranke ermitteln: $\Theta$-Notation}

\begin{compactitem}
\item $f(n) \in \Theta(g(n))$: $f(n)$ wächst ebenso schnell wie $g(n)$
\item $\Theta(g(n)) = \{f(n) |
\exists c_1 > 0
\exists c_2 > 0
\exists n_0 > 0
\forall n \geq n_0 :
 0 \leq c 1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \}$
\end{compactitem}

\subsection{„Sandwich-Gedanke“}

\begin{compactitem}
\item $\mathcal{O}$ ist eine (möglicherweise viel zu große obere
Schranke).

\item $\Omega$ ist eine (möglicherweise viel zu kleine untere
Schranke). Beide helfen bei der Charakterisierung von Aufwänden nur
bedingt.

\item  Es gilt: $f(n) \in \Theta(g(n))$ genau dann, wenn $f(n) \in
\mathcal{0}(g(n))$ und $f(n) \in \Omega(g(n))$.

\item Aufwandsabschätzungen mit Hilfe von $\Theta$ wären ideal, aber
leider lassen sich Beweise oft nur schwer führen.
\end{compactitem}

Daher werden oft in der Literatur nur $\mathcal{O}$-Abschätzungen
verwendet, aber mit der Maßgabe, dass die kleinstmögliche obere Schranke
angegeben wird.
\footcite[Seite 37 (PDF 26)]{aud:fs:2}

\subsection{Vergleichsbasiertes Sortieren}

Allgemeine Verfahren basieren auf dem paarweisen Vergleich der zu
sortierenden Elemente, ob das eine Element „kleiner“ als, „größer“ als
oder „gleich(groß)“ wie das andere Element ist. Bei der
Komplexitätsanalyse wird davon ausgegangen, dass der Aufwand zum
Vergleich zweier Elemente konstant ist.
\footcite{wiki:sortierverfahren}
\footcite[Seite 35 (PDF 24)]{aud:fs:2}

\begin{center}
\begin{tabular}{llll}
\textbf{Sortierverfahren} &
\textbf{Best-Case} &
\textbf{Average-Case} &
\textbf{Worst-Case}
\\\hline

Binary Tree Sort &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n \cdot \log (n) )$ &
$\Theta(n^2)$
\\\hline

Bubblesort &
$\Theta(n)$ &
$\Theta(n^2)$ &
$\Theta(n^2)$
\\\hline

Heapsort &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n \cdot \log (n) )$
\\\hline

Insertionsort &
$ \Theta(n)$ &
$\Theta(n^2)$ &
$\Theta(n^2)$
\\\hline

Mergesort &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n \cdot \log (n) )$
\\\hline

Quicksort &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n \cdot \log (n) )$ &
$\Theta( n^2 )$
\\\hline

Selectionsort &
$\Theta( n^2 )$ &
$\Theta( n^2 )$ &
$\Theta( n^2 )$
\\\hline
\end{tabular}
\end{center}

\begin{liExkurs}[Logarithmus]
Der Logarithmus zur Basis $b$ ist die Umkehrfunktion der allgemeinen
Exponentialfunktion zur positiven Basis $b$:

$x \mapsto b^x.$

Die Funktionen $b^x$ und $\log_b x$ sind also Umkehrfunktionen
voneinander, \dh Logarithmieren macht Exponenzieren rückgängig und
umgekehrt:

$b^{\log_b x} = x \quad \text{und} \quad \log_b(b^x) = x.$

Der natürliche Logarithmus ergibt sich mit der Basis $b=\mathrm{e}$,
wobei $\mathrm{e} = 2{,}718281828459\ldots$ die Eulersche Zahl ist.
\footcite[Definition / Als Umkehrfunktion der Exponentialfunktion]{wiki:logarithmus}
\end{liExkurs}
\begin{liExkurs}[asymptotische Abschätzung]%
Unter der Zeitkomplexität eines Problems wird in der Informatik die
Anzahl der Rechenschritte verstanden, die ein optimaler Algorithmus zur
Lösung dieses Problems benötigt, in Abhängigkeit von der Länge der
Eingabe. Man spricht hier auch von der \emph{asymptotischen Laufzeit}
und meint damit, in Anlehnung an eine \emph{Asymptote}, das
Zeitverhalten des Algorithmus für eine \emph{potenziell unendlich große
Eingabemenge}.

Es interessiert also \emph{nicht} der \emph{Zeitaufwand eines konkreten
Programms} auf einem bestimmten Computer, sondern viel mehr, wie der
Zeitbedarf wächst, wenn mehr Daten zu verarbeiten sind, also \zB ob
sich der Aufwand für die doppelte Datenmenge verdoppelt oder quadriert
(Skalierbarkeit).
\footcite{wiki:zeitkomplexitaet}
\end{liExkurs}

\literatur
\end{document}
